{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ab6261-8bf6-4dd2-96d8-e2ca21f8a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9836438-9c0d-48a9-8ade-5b0c3f73971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "D_MODEL = 512 # Dimension of input token representation - embeddings\n",
    "VOCAB_SIZE = 50000 # Number of tokens in vocabulary\n",
    "MAX_SEQ_LEN = 64 # Maximum Sequence Length of Input and Output Sequence\n",
    "NUM_LAYERS = 2 # Number of encoder and decoder stacks\n",
    "ATTN_DROPOUT = 0.2 # Quantify the dropout of how much in attention\n",
    "EPS = 1e-6 # A small value to avoid zero division error while normalization\n",
    "FF_DROPOUT = 0.2 # How much dropout in FFN``\n",
    "NUM_HEADS = 8 # Number of attention heads\n",
    "RES_DROPOUT = 0.2 # Dropout for residual connection\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRAD_CLIP = 2.0\n",
    "TOKENIZER_PATH = \"tokenizer.json\" # Tokenizer path\n",
    "DATASET_ID = \"emotion_data.parquet\" # Dataset path\n",
    "SRC_CLN_NAME = \"text\" # Source column name\n",
    "TGT_CLN_NAME = \"label\" # Target column name\n",
    "LOG_DIR = \"logs/v1\" # Logging dir\n",
    "BATCH_SIZE = 10 # Number of samples per batch\n",
    "LR = 1e-4 # Learning Rate for optimizer\n",
    "NUM_EPOCHS = 100# Number of training epochs\n",
    "MODEL_SAVE_PATH = \"transformer_classifier\" # Model save path\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42b3037-ae00-46bd-a56f-6059898248bb",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed22979-98be-4e0c-bf3e-5427a524c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "# Token embedding\n",
    "\n",
    "class TokenEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(\n",
    "                                        num_embeddings=vocab_size,\n",
    "                                        embedding_dim=d_model\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        token_embedding = self.embedding(x)\n",
    "        return token_embedding\n",
    "a  = torch.ones((BATCH_SIZE, MAX_SEQ_LEN-2), dtype = torch.int64)\n",
    "TE = TokenEmbeddings(D_MODEL, VOCAB_SIZE)\n",
    "print(TE(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e5a47b-a25b-4cca-bfb8-3a25212b43bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "# positional embedding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, seq_len: int, dropout_p: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        position_encodings = torch.zeros(self.seq_len, self.d_model)\n",
    "        positions = torch.arange(0, self.seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        even_odd_i = torch.arange(0, self.d_model, 2).float()\n",
    "        div_freqs_term = torch.pow(10000, even_odd_i/d_model)\n",
    "        position_encodings[:, 0::2] = torch.sin(positions*div_freqs_term)\n",
    "        position_encodings[:, 1::2] = torch.cos(positions*div_freqs_term)\n",
    "        position_encodings = position_encodings.unsqueeze(0)\n",
    "        self.register_buffer('position_encodings', position_encodings)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + (self.position_encodings[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        pos_encoding = self.dropout(x)\n",
    "        return pos_encoding\n",
    "a  = torch.ones((BATCH_SIZE, MAX_SEQ_LEN-2, D_MODEL))\n",
    "PE = PositionalEncoding(D_MODEL, MAX_SEQ_LEN, FF_DROPOUT)\n",
    "print(PE(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f6ff8b-7a59-4b21-b02a-f00f640d490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "# input embeddings\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = D_MODEL\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.seq_len = MAX_SEQ_LEN\n",
    "        self.dropout_p = FF_DROPOUT\n",
    "        self.token_embedding = TokenEmbeddings(self.d_model, self.vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model, self.seq_len, self.dropout_p)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        token_embed_x = self.token_embedding(x)\n",
    "        inp_embedding = self.positional_encoding(token_embed_x)\n",
    "        return inp_embedding\n",
    "\n",
    "a  = torch.ones((BATCH_SIZE, MAX_SEQ_LEN-2), dtype = torch.int64)\n",
    "IE = InputEmbeddings()\n",
    "print(IE(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea31e2ee-d593-4218-9903-db3a4b2bd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOU 0\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "# load training dataset for tokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_id = \"stanfordnlp/imdb\"\n",
    "tokenizer_dataset = load_dataset(data_id, split=\"train\")\n",
    "print(tokenizer_dataset['text'][0][:20], tokenizer_dataset['label'][0])\n",
    "print(set(tokenizer_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8c029e-c763-4693-a46e-4e3096ffab3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I rented I AM CURIOU\n"
     ]
    }
   ],
   "source": [
    "# make iterator for the dataset\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "def tokenizer_dataset_iterator(dataset):\n",
    "    for data in dataset:\n",
    "        yield data[\"text\"]\n",
    "for a in tokenizer_dataset_iterator(tokenizer_dataset):\n",
    "    print(a[:20])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40fea97a-b29b-4623-b2fb-12419cd77712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[15, 1718, 15, 14456, 34507, 20, 26873, 3228]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "if path.exists(\"tokenizer.json\"):\n",
    "    tokenizer = Tokenizer.from_file(\"tokenizer.json\")\n",
    "else:\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer_trainer = WordLevelTrainer(vocab_size=VOCAB_SIZE, special_tokens=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"], min_frequency=2)\n",
    "    tokenizer.train_from_iterator(tokenizer_dataset_iterator(tokenizer_dataset), trainer=tokenizer_trainer)\n",
    "    tokenizer.save(\"tokenizer.json\")\n",
    "print(tokenizer.encode(\"[PAD] [SOS] [EOS] [UNK]\").ids)\n",
    "print(tokenizer.encode(a[:30]).ids)\n",
    "print(tokenizer.encode(\"7569\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f27f1e5-0132-4f03-8568-35124a8d3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset_id: str, tokenizer: Tokenizer, max_seq_len: int,  src_cln_name: str, tgt_cln_name: str, split: str):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_cln_name = src_cln_name\n",
    "        self.tgt_cln_name = tgt_cln_name\n",
    "        self.dataset_id = dataset_id\n",
    "        #self.dataset = HFDataset.from_pandas(pd.read_parquet(DATASET_ID).sample(frac=1))\n",
    "        self.dataset = load_dataset(self.dataset_id, split=split)\n",
    "        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")]).to(torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")]).to(torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")]).to(torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = self.dataset[index]\n",
    "        src_data = data[self.src_cln_name]\n",
    "        tgt_data = data[self.tgt_cln_name]\n",
    "        encoder_inp_tokens = torch.tensor(self.tokenizer.encode(src_data).ids).to(torch.int64)\n",
    "        decoder_inp_tokens = torch.tensor(self.tokenizer.encode(str(tgt_data)).ids).to(torch.int64)\n",
    "\n",
    "        if encoder_inp_tokens.size(0) > self.max_seq_len-2:\n",
    "            encoder_inp_tokens = encoder_inp_tokens[:self.max_seq_len-2]\n",
    "\n",
    "        encoder_num_padding_tokens = self.max_seq_len - len(encoder_inp_tokens) - 2\n",
    "        decoder_num_padding_tokens = self.max_seq_len - len(decoder_inp_tokens) - 1\n",
    "\n",
    "        encoder_inp_pad_tokens = torch.tensor([self.pad_token] * encoder_num_padding_tokens).to(torch.int64)\n",
    "        decoder_inp_pad_tokens = torch.tensor([self.pad_token] * decoder_num_padding_tokens).to(torch.int64) \n",
    "        label_pad_tokens = torch.tensor([self.pad_token] * decoder_num_padding_tokens).to(torch.int64)\n",
    "\n",
    "        encoder_inp_tokens = torch.cat([self.sos_token, encoder_inp_tokens, self.eos_token, encoder_inp_pad_tokens], dim=0)\n",
    "        decoder_inpp_tokens = torch.cat([self.sos_token, decoder_inp_tokens, decoder_inp_pad_tokens], dim=0)\n",
    "        label_tokens = torch.cat([decoder_inp_tokens, self.eos_token, label_pad_tokens], dim=0)\n",
    "        \n",
    "        encoder_attn_mask = (encoder_inp_tokens != self.pad_token).unsqueeze(0).unsqueeze(0).to(torch.int64)\n",
    "        auto_regressive_mask = torch.tril(torch.ones((1, decoder_inpp_tokens.size(0), decoder_inpp_tokens.size(0))), diagonal=0).type(torch.int64)\n",
    "        decoder_attn_mask = ((decoder_inpp_tokens != self.pad_token).unsqueeze(0).to(torch.int64) & auto_regressive_mask).to(torch.int64)\n",
    "        \n",
    "        model_inp = {}\n",
    "        model_inp[\"encoder_input_ids\"] = encoder_inp_tokens\n",
    "        model_inp[\"decoder_input_ids\"] = decoder_inpp_tokens\n",
    "        model_inp[\"encoder_attention_mask\"] = encoder_attn_mask\n",
    "        model_inp[\"decoder_attention_mask\"] = decoder_attn_mask\n",
    "        model_inp[\"labels\"] = label_tokens\n",
    "        model_inp[\"source\"] = src_data\n",
    "        model_inp[\"target\"] = tgt_data\n",
    "        return model_inp\n",
    "\n",
    "CD_train = ClassificationDataset(data_id, tokenizer, MAX_SEQ_LEN, \"text\", \"label\", \"train\")\n",
    "CD_test = ClassificationDataset(data_id, tokenizer, MAX_SEQ_LEN, \"text\", \"label\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898e9ee7-9fca-40bf-b4a8-50bf773ef899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0],\n",
       "         [1, 1, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CD_train[0][\"decoder_attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f794fb-43a4-47c0-b6c3-5e93bd42ac5f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480958d2-5c6a-4c97-84ae-ac628563ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Fully connected\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, ff_dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ff_dropout = ff_dropout\n",
    "        self.d_ff = self.d_model * 4\n",
    "        self.linear_1 = nn.Linear(self.d_model, self.d_ff)\n",
    "        self.linear_2 = nn.Linear(self.d_ff, self.d_model)\n",
    "        self.ff_dropout_layer = nn.Dropout(self.ff_dropout)\n",
    "        self.activation_fn = torch.relu\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        linear_1_out = self.linear_1(x)\n",
    "        linear_1_out = self.activation_fn(linear_1_out)\n",
    "        linear_1_out = self.ff_dropout_layer(linear_1_out)\n",
    "        ff_out = self.linear_2(linear_1_out)\n",
    "        return ff_out\n",
    "        \n",
    "FF = FeedForward(D_MODEL, FF_DROPOUT)\n",
    "a  = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "print(FF(a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43597624-4fbb-447f-a0c3-6f9def726055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int, attn_dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def attention_calculation(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor):\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            attention_scores = self.dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        query = self.wq(q)\n",
    "        key = self.wk(k)\n",
    "        value = self.wv(v)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        x, self.attention_scores = self.attention_calculation(query, key, value, attn_mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.d_model)  \n",
    "        attn_out = self.wo(x)\n",
    "        return attn_out\n",
    "        \n",
    "q = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "k = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "v = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "MHA = MultiHeadAttention(D_MODEL, NUM_HEADS, ATTN_DROPOUT)\n",
    "print(MHA(q, k, v, None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a18c9c-685f-4f8e-91f1-d0d9d70e12a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Residual connection and layernorm\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        x_normalized = (x - mean)/(std + self.eps) \n",
    "        layer_norm_out = self.alpha * x_normalized + self.beta\n",
    "        return layer_norm_out\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, res_dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout_layer = nn.Dropout(res_dropout)\n",
    "        self.layer_norm_layer = LayerNorm(d_model, 1e-6)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        sublayer_out = self.dropout_layer(sublayer(self.layer_norm_layer(x)))\n",
    "        add_layer_norm_out = x + sublayer_out\n",
    "        return add_layer_norm_out\n",
    "        \n",
    "a = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "RES = ResidualConnection(D_MODEL, RES_DROPOUT)\n",
    "print(RES(a, FF).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579ba190-19ba-421d-ad50-4c4db0e973b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# encoder\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = D_MODEL\n",
    "        self.num_heads = NUM_HEADS\n",
    "        self.attn_dropout = ATTN_DROPOUT\n",
    "        self.ff_dropout = FF_DROPOUT\n",
    "        self.res_dropout = RES_DROPOUT\n",
    "        self.self_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)\n",
    "        self.feed_forward = FeedForward(self.d_model, self.ff_dropout)\n",
    "        self.res_connection1 = ResidualConnection(self.d_model, self.res_dropout)\n",
    "        self.res_connection2 = ResidualConnection(self.d_model, self.res_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, src_attn_mask: torch.Tensor):\n",
    "        attn_out = self.res_connection1(x, lambda x: self.self_attention(x, x, x, src_attn_mask))\n",
    "        enc_block_out = self.res_connection2(attn_out, self.feed_forward)\n",
    "        return enc_block_out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock() for _ in range(num_layers)])\n",
    "        self.layer_norm = LayerNorm(d_model, EPS)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, attn_mask: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        encoder_out = self.layer_norm(x)\n",
    "        return encoder_out\n",
    "\n",
    "a = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "ENC = Encoder(D_MODEL, NUM_LAYERS)\n",
    "print(ENC(a, None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba6fb5c-26ad-4d65-87f3-fa9d3f9db3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = D_MODEL\n",
    "        self.num_heads = NUM_HEADS\n",
    "        self.attn_dropout = ATTN_DROPOUT\n",
    "        self.ff_dropout = FF_DROPOUT\n",
    "        self.res_dropout = RES_DROPOUT\n",
    "        self.self_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)\n",
    "        self.cross_attention = MultiHeadAttention(self.d_model, self.num_heads, self.attn_dropout)\n",
    "        self.feed_forward = FeedForward(self.d_model, self.ff_dropout)\n",
    "        self.res_connection1 = ResidualConnection(self.d_model, self.res_dropout)\n",
    "        self.res_connection2 = ResidualConnection(self.d_model, self.res_dropout)\n",
    "        self.res_connection3 = ResidualConnection(self.d_model, self.res_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor, src_attn_mask: torch.Tensor, tgt_attn_mask: torch.Tensor):\n",
    "        self_attn_out = self.res_connection1(x, lambda x: self.self_attention(x, x, x, tgt_attn_mask))\n",
    "        cross_attn_out = self.res_connection2(self_attn_out, lambda self_attn_out: self.cross_attention(self_attn_out, encoder_out, encoder_out, src_attn_mask))\n",
    "        dec_block_out = self.res_connection3(cross_attn_out, self.feed_forward)\n",
    "        return dec_block_out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock() for _ in range(num_layers)])\n",
    "        self.layer_norm = LayerNorm(d_model, EPS)\n",
    "\n",
    "    def forward(self, x:torch.Tensor, encoder_out: torch.Tensor, src_attn_mask: torch.Tensor, tgt_attn_mask: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_out, src_attn_mask, tgt_attn_mask)\n",
    "        decoder_out = self.layer_norm(x)\n",
    "        return decoder_out\n",
    "\n",
    "a = torch.rand((BATCH_SIZE, MAX_SEQ_LEN, D_MODEL))\n",
    "DEC = Decoder(D_MODEL, NUM_LAYERS)\n",
    "print(DEC(a, a, None, None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81697735-880a-43a3-a2a9-011305d689cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 50000])\n"
     ]
    }
   ],
   "source": [
    "# transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_model = D_MODEL\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.inp_embedding = InputEmbeddings()\n",
    "        self.encoder = Encoder(self.d_model, self.num_layers)\n",
    "        self.decoder = Decoder(self.d_model, self.num_layers)\n",
    "        self.projection = nn.Linear(self.d_model, self.vocab_size)\n",
    "        self.__init__weights()\n",
    "\n",
    "    def forward(self, src_x, src_attn_mask, tgt_x, tgt_attn_mask):\n",
    "        src_embed = self.inp_embedding(src_x)\n",
    "        tgt_embed = self.inp_embedding(tgt_x)\n",
    "        encoder_out = self.encoder(src_embed, src_attn_mask)\n",
    "        decoder_out = self.decoder(tgt_embed, encoder_out, src_attn_mask, tgt_attn_mask)\n",
    "        transformer_out = self.projection(decoder_out)\n",
    "        return transformer_out\n",
    "    \n",
    "    def __init__weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "T = Transformer()\n",
    "a = torch.ones((BATCH_SIZE, MAX_SEQ_LEN), dtype = torch.int64)\n",
    "b = torch.ones((BATCH_SIZE, MAX_SEQ_LEN), dtype = torch.int64)\n",
    "print(T(a,None,b,None).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe898b3-3943-475f-8d33-30f35cb019a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss computation\n",
    "\n",
    "def compute_loss(logits: torch.Tensor, labels: torch.Tensor, loss_fn):\n",
    "    logits = logits.view(-1, VOCAB_SIZE)\n",
    "    labels = labels.view(-1)\n",
    "    loss = loss_fn(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89ce5da3-1cb2-4a75-9718-e36a7c56d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Initialize model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 0: 100%|██████████| 2500/2500 [01:49<00:00, 22.80it/s]\n",
      "Training loss 0: 100%|██████████| 2500/2500 [00:44<00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9935561979293825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 0: 100%|██████████| 2500/2500 [00:44<00:00, 56.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9935563274383545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 2500/2500 [01:53<00:00, 22.03it/s]\n",
      "Training loss 1: 100%|██████████| 2500/2500 [00:45<00:00, 54.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5594883009195327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 1: 100%|██████████| 2500/2500 [00:44<00:00, 55.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5594879052639008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2: 100%|██████████| 2500/2500 [01:54<00:00, 21.90it/s]\n",
      "Training loss 2: 100%|██████████| 2500/2500 [00:46<00:00, 53.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3798830001115799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 2: 100%|██████████| 2500/2500 [00:45<00:00, 55.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.379882633960247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3: 100%|██████████| 2500/2500 [01:54<00:00, 21.92it/s]\n",
      "Training loss 3: 100%|██████████| 2500/2500 [00:46<00:00, 53.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3567025749921799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 3: 100%|██████████| 2500/2500 [00:45<00:00, 54.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3567024514079094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4: 100%|██████████| 2500/2500 [01:53<00:00, 21.96it/s]\n",
      "Training loss 4: 100%|██████████| 2500/2500 [00:47<00:00, 52.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35143837517499926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 4: 100%|██████████| 2500/2500 [00:45<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3514383689522743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5: 100%|██████████| 2500/2500 [01:53<00:00, 21.94it/s]\n",
      "Training loss 5: 100%|██████████| 2500/2500 [00:47<00:00, 52.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349987625169754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 5: 100%|██████████| 2500/2500 [00:45<00:00, 54.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3499876247882843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6: 100%|██████████| 2500/2500 [01:54<00:00, 21.86it/s]\n",
      "Training loss 6: 100%|██████████| 2500/2500 [00:47<00:00, 52.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3548777103424072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 6: 100%|██████████| 2500/2500 [00:45<00:00, 55.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3548777103424072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7: 100%|██████████| 2500/2500 [01:54<00:00, 21.85it/s]\n",
      "Training loss 7: 100%|██████████| 2500/2500 [00:47<00:00, 53.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3495593305110931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 7: 100%|██████████| 2500/2500 [00:45<00:00, 55.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34955933690071106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8: 100%|██████████| 2500/2500 [01:54<00:00, 21.79it/s]\n",
      "Training loss 8: 100%|██████████| 2500/2500 [00:47<00:00, 52.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34917253255844116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 8: 100%|██████████| 2500/2500 [00:45<00:00, 55.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34917253255844116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9: 100%|██████████| 2500/2500 [01:53<00:00, 21.94it/s]\n",
      "Training loss 9: 100%|██████████| 2500/2500 [00:47<00:00, 52.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502719679951668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 9: 100%|██████████| 2500/2500 [00:45<00:00, 55.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502719849348068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10: 100%|██████████| 2500/2500 [01:51<00:00, 22.50it/s]\n",
      "Training loss 10: 100%|██████████| 2500/2500 [00:41<00:00, 59.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3496589412331581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 10: 100%|██████████| 2500/2500 [00:40<00:00, 61.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34965893626213074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 11: 100%|██████████| 2500/2500 [01:47<00:00, 23.19it/s]\n",
      "Training loss 11: 100%|██████████| 2500/2500 [00:42<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.351377580678463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 11: 100%|██████████| 2500/2500 [00:41<00:00, 59.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35137757643461226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 12: 100%|██████████| 2500/2500 [01:47<00:00, 23.33it/s]\n",
      "Training loss 12: 100%|██████████| 2500/2500 [00:41<00:00, 59.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3517321347475052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 12: 100%|██████████| 2500/2500 [00:40<00:00, 61.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35173211991786957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 13: 100%|██████████| 2500/2500 [01:48<00:00, 23.00it/s]\n",
      "Training loss 13: 100%|██████████| 2500/2500 [00:42<00:00, 59.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35230426869392395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 13: 100%|██████████| 2500/2500 [00:42<00:00, 59.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35230427980422974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 14: 100%|██████████| 2500/2500 [01:51<00:00, 22.39it/s]\n",
      "Training loss 14: 100%|██████████| 2500/2500 [00:45<00:00, 54.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35346810907125475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 14: 100%|██████████| 2500/2500 [00:44<00:00, 56.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.353468120098114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 15: 100%|██████████| 2500/2500 [01:51<00:00, 22.41it/s]\n",
      "Training loss 15: 100%|██████████| 2500/2500 [00:45<00:00, 54.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3534524193048477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 15: 100%|██████████| 2500/2500 [00:44<00:00, 55.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3534524142742157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 16: 100%|██████████| 2500/2500 [01:51<00:00, 22.38it/s]\n",
      "Training loss 16: 100%|██████████| 2500/2500 [00:46<00:00, 53.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35348254425525666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 16: 100%|██████████| 2500/2500 [00:44<00:00, 56.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35348252952098846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 17: 100%|██████████| 2500/2500 [01:49<00:00, 22.77it/s]\n",
      "Training loss 17: 100%|██████████| 2500/2500 [00:41<00:00, 59.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35254865882396696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 17: 100%|██████████| 2500/2500 [00:40<00:00, 62.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35254864394664764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 18: 100%|██████████| 2500/2500 [01:43<00:00, 24.15it/s]\n",
      "Training loss 18: 100%|██████████| 2500/2500 [00:40<00:00, 61.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35166993987560274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 18: 100%|██████████| 2500/2500 [00:39<00:00, 63.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35166993737220764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 19: 100%|██████████| 2500/2500 [01:50<00:00, 22.66it/s]\n",
      "Training loss 19: 100%|██████████| 2500/2500 [00:46<00:00, 53.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3516523240566254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 19: 100%|██████████| 2500/2500 [00:44<00:00, 56.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3516523241996765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 20: 100%|██████████| 2500/2500 [01:53<00:00, 22.04it/s]\n",
      "Training loss 20: 100%|██████████| 2500/2500 [00:47<00:00, 53.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35096586458683016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 20: 100%|██████████| 2500/2500 [00:44<00:00, 56.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35096585750579834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 21: 100%|██████████| 2500/2500 [01:53<00:00, 22.06it/s]\n",
      "Training loss 21: 100%|██████████| 2500/2500 [00:46<00:00, 53.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3529063175797462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 21: 100%|██████████| 2500/2500 [00:44<00:00, 56.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35290631651878357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 22: 100%|██████████| 2500/2500 [01:52<00:00, 22.18it/s]\n",
      "Training loss 22: 100%|██████████| 2500/2500 [00:46<00:00, 54.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3512192886829376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 22: 100%|██████████| 2500/2500 [00:40<00:00, 61.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3512192964553833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 23: 100%|██████████| 2500/2500 [01:46<00:00, 23.50it/s]\n",
      "Training loss 23: 100%|██████████| 2500/2500 [00:41<00:00, 59.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35145214676856995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 23: 100%|██████████| 2500/2500 [00:40<00:00, 61.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35145212709903717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 24: 100%|██████████| 2500/2500 [01:46<00:00, 23.51it/s]\n",
      "Training loss 24: 100%|██████████| 2500/2500 [00:41<00:00, 59.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3505292749285698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 24: 100%|██████████| 2500/2500 [00:40<00:00, 62.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3505292683839798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 25: 100%|██████████| 2500/2500 [01:46<00:00, 23.45it/s]\n",
      "Training loss 25: 100%|██████████| 2500/2500 [00:41<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35024201134443284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 25: 100%|██████████| 2500/2500 [00:40<00:00, 62.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.350242018699646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 26: 100%|██████████| 2500/2500 [01:46<00:00, 23.41it/s]\n",
      "Training loss 26: 100%|██████████| 2500/2500 [00:41<00:00, 59.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35066472754478456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 26: 100%|██████████| 2500/2500 [00:40<00:00, 61.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35066473484039307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 27: 100%|██████████| 2500/2500 [01:46<00:00, 23.52it/s]\n",
      "Training loss 27: 100%|██████████| 2500/2500 [00:42<00:00, 59.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3502116281867027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 27: 100%|██████████| 2500/2500 [00:40<00:00, 61.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35021162033081055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 28: 100%|██████████| 2500/2500 [01:46<00:00, 23.56it/s]\n",
      "Training loss 28: 100%|██████████| 2500/2500 [00:41<00:00, 60.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498586869597435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 28: 100%|██████████| 2500/2500 [00:40<00:00, 61.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34985868632793427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 29: 100%|██████████| 2500/2500 [01:46<00:00, 23.53it/s]\n",
      "Training loss 29: 100%|██████████| 2500/2500 [00:41<00:00, 59.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34986711267232895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 29: 100%|██████████| 2500/2500 [00:43<00:00, 57.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498671054840088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 30: 100%|██████████| 2500/2500 [01:53<00:00, 22.09it/s]\n",
      "Training loss 30: 100%|██████████| 2500/2500 [00:47<00:00, 53.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349857210958004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 30: 100%|██████████| 2500/2500 [00:44<00:00, 55.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498572111129761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 31: 100%|██████████| 2500/2500 [01:52<00:00, 22.19it/s]\n",
      "Training loss 31: 100%|██████████| 2500/2500 [00:46<00:00, 53.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3495367481231689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 31: 100%|██████████| 2500/2500 [00:44<00:00, 55.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3495367467403412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 32: 100%|██████████| 2500/2500 [01:50<00:00, 22.72it/s]\n",
      "Training loss 32: 100%|██████████| 2500/2500 [00:41<00:00, 59.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3491070977807045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 32: 100%|██████████| 2500/2500 [00:40<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3491070866584778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 33: 100%|██████████| 2500/2500 [01:48<00:00, 23.15it/s]\n",
      "Training loss 33: 100%|██████████| 2500/2500 [00:42<00:00, 59.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35021162773370745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 33: 100%|██████████| 2500/2500 [00:40<00:00, 61.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35021162033081055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 34: 100%|██████████| 2500/2500 [01:46<00:00, 23.55it/s]\n",
      "Training loss 34: 100%|██████████| 2500/2500 [00:42<00:00, 59.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34918051958084106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 34: 100%|██████████| 2500/2500 [00:40<00:00, 61.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34918051958084106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 35: 100%|██████████| 2500/2500 [01:46<00:00, 23.46it/s]\n",
      "Training loss 35: 100%|██████████| 2500/2500 [00:42<00:00, 59.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34956409375667574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 35: 100%|██████████| 2500/2500 [00:40<00:00, 61.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3495640903711319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 36: 100%|██████████| 2500/2500 [01:47<00:00, 23.18it/s]\n",
      "Training loss 36: 100%|██████████| 2500/2500 [00:42<00:00, 58.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498623117089272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 36: 100%|██████████| 2500/2500 [00:43<00:00, 57.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3498622924089432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 37: 100%|██████████| 2500/2500 [01:54<00:00, 21.86it/s]\n",
      "Training loss 37: 100%|██████████| 2500/2500 [00:47<00:00, 53.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3492669463157654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 37: 100%|██████████| 2500/2500 [00:45<00:00, 55.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3492669463157654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 38: 100%|██████████| 2500/2500 [01:51<00:00, 22.39it/s]\n",
      "Training loss 38: 100%|██████████| 2500/2500 [00:42<00:00, 59.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34860809851884844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 38: 100%|██████████| 2500/2500 [00:40<00:00, 61.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3486081063747406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 39: 100%|██████████| 2500/2500 [01:48<00:00, 22.98it/s]\n",
      "Training loss 39: 100%|██████████| 2500/2500 [00:41<00:00, 59.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3486627564311027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 39: 100%|██████████| 2500/2500 [00:41<00:00, 60.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34866274893283844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 40: 100%|██████████| 2500/2500 [01:49<00:00, 22.93it/s]\n",
      "Training loss 40: 100%|██████████| 2500/2500 [00:42<00:00, 59.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485300726056099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 40: 100%|██████████| 2500/2500 [00:42<00:00, 59.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485300689935684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 41: 100%|██████████| 2500/2500 [01:51<00:00, 22.51it/s]\n",
      "Training loss 41: 100%|██████████| 2500/2500 [00:47<00:00, 53.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3495653288245201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 41: 100%|██████████| 2500/2500 [00:45<00:00, 54.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349565327167511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 42: 100%|██████████| 2500/2500 [01:53<00:00, 21.95it/s]\n",
      "Training loss 42: 100%|██████████| 2500/2500 [00:47<00:00, 52.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34884363782405853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 42: 100%|██████████| 2500/2500 [00:45<00:00, 54.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34884363412857056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 43: 100%|██████████| 2500/2500 [01:54<00:00, 21.79it/s]\n",
      "Training loss 43: 100%|██████████| 2500/2500 [00:47<00:00, 53.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3483759415984154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 43: 100%|██████████| 2500/2500 [00:45<00:00, 54.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34837594628334045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 44: 100%|██████████| 2500/2500 [01:54<00:00, 21.76it/s]\n",
      "Training loss 44: 100%|██████████| 2500/2500 [00:47<00:00, 52.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484070522069931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 44: 100%|██████████| 2500/2500 [00:45<00:00, 54.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.348407045006752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 45: 100%|██████████| 2500/2500 [01:54<00:00, 21.75it/s]\n",
      "Training loss 45: 100%|██████████| 2500/2500 [00:48<00:00, 51.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3483073907613754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 45: 100%|██████████| 2500/2500 [00:45<00:00, 55.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34830738604068756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 46: 100%|██████████| 2500/2500 [01:53<00:00, 22.07it/s]\n",
      "Training loss 46: 100%|██████████| 2500/2500 [00:47<00:00, 52.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34843650460243225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 46: 100%|██████████| 2500/2500 [00:45<00:00, 54.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34843650460243225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 47: 100%|██████████| 2500/2500 [01:55<00:00, 21.68it/s]\n",
      "Training loss 47: 100%|██████████| 2500/2500 [00:47<00:00, 52.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3486859465122223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 47: 100%|██████████| 2500/2500 [00:45<00:00, 55.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34868595004081726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 48: 100%|██████████| 2500/2500 [01:55<00:00, 21.68it/s]\n",
      "Training loss 48: 100%|██████████| 2500/2500 [00:47<00:00, 52.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34846379714012143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 48: 100%|██████████| 2500/2500 [00:46<00:00, 54.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484638035297394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 49: 100%|██████████| 2500/2500 [01:55<00:00, 21.71it/s]\n",
      "Training loss 49: 100%|██████████| 2500/2500 [00:47<00:00, 52.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34834082659482957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 49: 100%|██████████| 2500/2500 [00:45<00:00, 54.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34834083914756775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 50: 100%|██████████| 2500/2500 [01:54<00:00, 21.83it/s]\n",
      "Training loss 50: 100%|██████████| 2500/2500 [00:56<00:00, 43.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34955651050806047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 50: 100%|██████████| 2500/2500 [01:12<00:00, 34.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34955650568008423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 51: 100%|██████████| 2500/2500 [02:29<00:00, 16.69it/s]\n",
      "Training loss 51: 100%|██████████| 2500/2500 [00:45<00:00, 54.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482837270021439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 51: 100%|██████████| 2500/2500 [00:44<00:00, 55.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482837378978729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 52: 100%|██████████| 2500/2500 [01:54<00:00, 21.77it/s]\n",
      "Training loss 52: 100%|██████████| 2500/2500 [00:48<00:00, 52.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34845307577848433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 52: 100%|██████████| 2500/2500 [00:45<00:00, 54.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484530746936798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 53: 100%|██████████| 2500/2500 [01:54<00:00, 21.77it/s]\n",
      "Training loss 53: 100%|██████████| 2500/2500 [00:47<00:00, 52.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34921814895868303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 53: 100%|██████████| 2500/2500 [00:44<00:00, 55.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34921814501285553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 54: 100%|██████████| 2500/2500 [01:55<00:00, 21.72it/s]\n",
      "Training loss 54: 100%|██████████| 2500/2500 [00:47<00:00, 53.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3493185470342636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 54: 100%|██████████| 2500/2500 [00:45<00:00, 54.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34931856393814087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 55: 100%|██████████| 2500/2500 [01:54<00:00, 21.78it/s]\n",
      "Training loss 55: 100%|██████████| 2500/2500 [00:47<00:00, 52.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34876005226373674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 55: 100%|██████████| 2500/2500 [00:45<00:00, 55.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34876005351543427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 56: 100%|██████████| 2500/2500 [02:04<00:00, 20.07it/s]\n",
      "Training loss 56: 100%|██████████| 2500/2500 [01:12<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34833804731369017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 56: 100%|██████████| 2500/2500 [01:12<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3483380377292633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 57: 100%|██████████| 2500/2500 [02:27<00:00, 16.93it/s]\n",
      "Training loss 57: 100%|██████████| 2500/2500 [01:12<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482517600059509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 57: 100%|██████████| 2500/2500 [01:11<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482517600059509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 58: 100%|██████████| 2500/2500 [02:27<00:00, 16.91it/s]\n",
      "Training loss 58: 100%|██████████| 2500/2500 [01:12<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34826291382312774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 58: 100%|██████████| 2500/2500 [01:11<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482629060745239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 59: 100%|██████████| 2500/2500 [02:27<00:00, 16.91it/s]\n",
      "Training loss 59: 100%|██████████| 2500/2500 [01:12<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34914665184021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 59: 100%|██████████| 2500/2500 [01:11<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34914664924144745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 60: 100%|██████████| 2500/2500 [02:27<00:00, 16.93it/s]\n",
      "Training loss 60: 100%|██████████| 2500/2500 [01:12<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485894802212715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 60: 100%|██████████| 2500/2500 [01:12<00:00, 34.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3485894799232483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 61: 100%|██████████| 2500/2500 [02:27<00:00, 16.91it/s]\n",
      "Training loss 61: 100%|██████████| 2500/2500 [01:12<00:00, 34.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34844433906078337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 61: 100%|██████████| 2500/2500 [01:11<00:00, 34.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484443426132202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 62: 100%|██████████| 2500/2500 [02:27<00:00, 16.97it/s]\n",
      "Training loss 62: 100%|██████████| 2500/2500 [01:41<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3481858197927475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 62: 100%|██████████| 2500/2500 [02:33<00:00, 16.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34818580746650696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 63: 100%|██████████| 2500/2500 [8:20:17<00:00, 12.01s/it]       \n",
      "Training loss 63: 100%|██████████| 2500/2500 [00:41<00:00, 59.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34816652562618255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 63: 100%|██████████| 2500/2500 [00:48<00:00, 51.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3481665253639221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 64: 100%|██████████| 2500/2500 [01:44<00:00, 23.84it/s]\n",
      "Training loss 64: 100%|██████████| 2500/2500 [00:45<00:00, 55.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34868108456134794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 64: 100%|██████████| 2500/2500 [00:48<00:00, 51.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3486810773611069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 65: 100%|██████████| 2500/2500 [01:48<00:00, 23.03it/s]\n",
      "Training loss 65: 100%|██████████| 2500/2500 [00:41<00:00, 59.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34815850979089735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 65: 100%|██████████| 2500/2500 [00:40<00:00, 62.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34815850853919983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 66: 100%|██████████| 2500/2500 [01:47<00:00, 23.33it/s]\n",
      "Training loss 66: 100%|██████████| 2500/2500 [00:45<00:00, 54.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34827416051626203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 66: 100%|██████████| 2500/2500 [00:49<00:00, 50.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482741713523865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 67: 100%|██████████| 2500/2500 [02:03<00:00, 20.27it/s]\n",
      "Training loss 67: 100%|██████████| 2500/2500 [00:43<00:00, 57.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34816622672080993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 67: 100%|██████████| 2500/2500 [00:42<00:00, 58.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34816622734069824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 68: 100%|██████████| 2500/2500 [01:49<00:00, 22.93it/s]\n",
      "Training loss 68: 100%|██████████| 2500/2500 [00:43<00:00, 58.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3482855743288994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 68: 100%|██████████| 2500/2500 [01:01<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34828557074069977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 69: 100%|██████████| 2500/2500 [02:27<00:00, 16.90it/s]\n",
      "Training loss 69: 100%|██████████| 2500/2500 [01:12<00:00, 34.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34818718489408496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 69: 100%|██████████| 2500/2500 [01:12<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3481871783733368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 70: 100%|██████████| 2500/2500 [02:29<00:00, 16.75it/s]\n",
      "Training loss 70: 100%|██████████| 2500/2500 [01:12<00:00, 34.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3483403213262558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 70: 100%|██████████| 2500/2500 [01:11<00:00, 34.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34834033250808716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 71: 100%|██████████| 2500/2500 [02:29<00:00, 16.76it/s]\n",
      "Training loss 71: 100%|██████████| 2500/2500 [01:12<00:00, 34.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34816011786460876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 71: 100%|██████████| 2500/2500 [01:11<00:00, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34816011786460876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 72: 100%|██████████| 2500/2500 [02:29<00:00, 16.77it/s]\n",
      "Training loss 72: 100%|██████████| 2500/2500 [01:12<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484006125688553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing loss 72: 100%|██████████| 2500/2500 [01:11<00:00, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3484005928039551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 73:  92%|█████████▏| 2297/2500 [02:16<00:12, 16.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m tgt_attn_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     31\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 33\u001b[0m model_logits \u001b[38;5;241m=\u001b[39m model(src_x, src_attn_mask, tgt_x, tgt_attn_mask)\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(model_logits, labels, loss_fn )\n\u001b[0;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 19\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src_x, src_attn_mask, tgt_x, tgt_attn_mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m src_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_embedding(src_x)\n\u001b[0;32m     18\u001b[0m tgt_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minp_embedding(tgt_x)\n\u001b[1;32m---> 19\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_embed, src_attn_mask)\n\u001b[0;32m     20\u001b[0m decoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt_embed, encoder_out, src_attn_mask, tgt_attn_mask)\n\u001b[0;32m     21\u001b[0m transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(decoder_out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor, attn_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 31\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, attn_mask)\n\u001b[0;32m     32\u001b[0m     encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder_out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x, src_attn_mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, src_attn_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     18\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_connection1(x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(x, x, x, src_attn_mask))\n\u001b[1;32m---> 19\u001b[0m     enc_block_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_connection2(attn_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enc_block_out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[1;32m---> 26\u001b[0m     sublayer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_layer(sublayer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_layer(x)))\n\u001b[0;32m     27\u001b[0m     add_layer_norm_out \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m sublayer_out\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m add_layer_norm_out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 16\u001b[0m     linear_1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1(x)\n\u001b[0;32m     17\u001b[0m     linear_1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(linear_1_out)\n\u001b[0;32m     18\u001b[0m     linear_1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_dropout_layer(linear_1_out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "print(\"Loading the datasets...\")\n",
    "#CD_train = ClassificationDataset(data_id, tokenizer, MAX_SEQ_LEN, \"text\", \"label\", \"train\")\n",
    "#CD_test = ClassificationDataset(data_id, tokenizer, MAX_SEQ_LEN, \"text\", \"label\", \"test\")\n",
    "\n",
    "train_dl = DataLoader(CD_train, BATCH_SIZE, num_workers=0, pin_memory=True, shuffle=True)\n",
    "test_dl = DataLoader(CD_test, BATCH_SIZE, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(\"Initialize model...\")\n",
    "model = Transformer()\n",
    "model.to(DEVICE)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr = LR, weight_decay=WEIGHT_DECAY)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(opt, LR, epochs=NUM_EPOCHS, steps_per_epoch=len(train_dl))\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id('[PAD]')).to(\"cuda\")\n",
    "\n",
    "train_ = []\n",
    "test_ = []\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for batch in tqdm(train_dl, desc=f\"Training {e}\"):\n",
    "        \n",
    "        src_x = batch[\"encoder_input_ids\"].to(DEVICE)\n",
    "        src_attn_mask = batch[\"encoder_attention_mask\"].to(DEVICE)\n",
    "        tgt_x = batch[\"decoder_input_ids\"].to(DEVICE)\n",
    "        tgt_attn_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        model_logits = model(src_x, src_attn_mask, tgt_x, tgt_attn_mask)\n",
    "        \n",
    "        loss = compute_loss(model_logits, labels, loss_fn )\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_value_(model.parameters(), GRAD_CLIP)\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        sched.step()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch in tqdm(train_dl, desc=f\"Training loss {e}\"):\n",
    "            src_x = batch[\"encoder_input_ids\"].to(DEVICE)\n",
    "            src_attn_mask = batch[\"encoder_attention_mask\"].to(DEVICE)\n",
    "            tgt_x = batch[\"decoder_input_ids\"].to(DEVICE)\n",
    "            tgt_attn_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            \n",
    "            model_logits = model(src_x, src_attn_mask, tgt_x, tgt_attn_mask)\n",
    "            \n",
    "            loss = compute_loss(model_logits, labels, loss_fn )\n",
    "    \n",
    "            train_losses.append(loss.item())\n",
    "        print(sum(train_losses)/len(train_losses))\n",
    "        train_.append(sum(train_losses)/len(train_losses))\n",
    "        for batch in tqdm(test_dl, desc=f\"Testing loss {e}\"):\n",
    "            src_x = batch[\"encoder_input_ids\"].to(DEVICE)\n",
    "            src_attn_mask = batch[\"encoder_attention_mask\"].to(DEVICE)\n",
    "            tgt_x = batch[\"decoder_input_ids\"].to(DEVICE)\n",
    "            tgt_attn_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "            \n",
    "            model_logits = model(src_x, src_attn_mask, tgt_x, tgt_attn_mask)\n",
    "            \n",
    "            loss = compute_loss(model_logits, labels, loss_fn )\n",
    "\n",
    "            test_losses.append(loss.item())\n",
    "        print(sum(test_losses)/len(test_losses))\n",
    "        test_.append(sum(test_losses)/len(test_losses))\n",
    "    # save checkpoints\n",
    "    ckpt_path = \"checkpoints/\"+f\"epoch-{e}-checkpoint.pth\"\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": opt.state_dict(),\n",
    "        \"scheduler\": sched.state_dict(),\n",
    "    }, ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20212920-7fca-473f-b032-bcafead565fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1'], 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "n = 17000\n",
    "sentence, label = CD_test[n]['source'], CD_test[n]['target']\n",
    "def classify_sentence(model, sentence, tokenizer):\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    source = tokenizer.encode(sentence).ids[:MAX_SEQ_LEN - 2]\n",
    "            \n",
    "    source = torch.cat([\n",
    "        torch.tensor([tokenizer.token_to_id('[SOS]')], dtype=torch.int64), \n",
    "        torch.tensor(source, dtype=torch.int64),\n",
    "        torch.tensor([tokenizer.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "        torch.tensor([tokenizer.token_to_id('[PAD]')] * (MAX_SEQ_LEN - len(source) - 2), dtype=torch.int64)\n",
    "    ], dim=0).to(DEVICE)\n",
    "    source_mask = (source != tokenizer.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(DEVICE)\n",
    "    \n",
    "    sos_idx = tokenizer.token_to_id(\"[SOS]\")\n",
    "    eos_idx = tokenizer.token_to_id(\"[EOS]\")\n",
    "    \n",
    "    decoder_inp = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(DEVICE)\n",
    "    \n",
    "    while True:\n",
    "        if decoder_inp.size(1) == MAX_SEQ_LEN:\n",
    "            break\n",
    "        tgt_attn_mask = torch.tril(torch.ones((1, decoder_inp.size(1), decoder_inp.size(1))), diagonal=0).type(torch.int64).type_as(source_mask).to(DEVICE)\n",
    "    \n",
    "        out = model(source, source_mask, decoder_inp, tgt_attn_mask)\n",
    "    \n",
    "        logits = torch.softmax(out, dim=2)\n",
    "    \n",
    "        next_word_logit_argmax = torch.argmax(logits, dim=2)\n",
    "        next_word_logit_argmax = next_word_logit_argmax[0][-1]\n",
    "    \n",
    "        decoder_inp = torch.cat([decoder_inp, torch.empty(1, 1).type_as(source).fill_(next_word_logit_argmax.item()).to(DEVICE)], dim=1)\n",
    "        \n",
    "        if next_word_logit_argmax == eos_idx:\n",
    "            break\n",
    "    model_out = tokenizer.decode_batch(decoder_inp.tolist())\n",
    "    return model_out\n",
    "model_out = classify_sentence(model, sentence, tokenizer)\n",
    "model_out, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d7c231c-9240-438f-afee-65585368c445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_sentence(model, \"It was superb movie\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49ffc7e8-55ad-4d57-bda9-46d25b0e433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Having read the other comment about this superb piece of TV drama I felt compelled to balance things a little. If you like you murders, to be signature and serial, and your cops to be British, and shout a lot, and the gore to be bloody and have a religious slant then this hits every button. Not quite enough 'gov'ing to put the shouting into the Sweeney's rarefied heights, but otherwise highly rated. Ken Stott is excellent as the 'cop on the edge' and the guest stars are also well cast, including Edward Woodward and Art Malik. Recommended. (In response to the earlier comments, although I accept that 'Red' would not 'normally' drive away from a hit and run, he had just witnessed his brother arrested for murder, and I am fairly sure he does not see the boy move.)\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CD_test[n]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "171a1ad3-ba4d-4626-8ec0-2c0acd55f383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f033d33110>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(train_)\n",
    "plt.plot(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34d442-29a5-4d58-ab0a-9fa93588a8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
